{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2c44190",
   "metadata": {},
   "source": [
    "# Solving Cartpole Balancing with Amazon SageMaker and Ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e4355e",
   "metadata": {},
   "source": [
    "![](figures/cartpole.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578e8704",
   "metadata": {},
   "source": [
    "---\n",
    "## Introduction\n",
    "\n",
    "In this notebook we'll start from the cart-pole balancing problem, where a pole is attached by an un-actuated joint to a cart, moving along a frictionless track. Instead of applying control theory to solve the problem, this example shows how to solve the problem with reinforcement learning on Amazon SageMaker and Ray RLlib. You can choose either TensorFlow or PyTorch as your underlying DL framework.\n",
    "\n",
    "(For a similar example using Coach library, see this [link](../rl_cartpole_coach/rl_cartpole_coach_gymEnv.ipynb). Another Cart-pole example using Coach library and offline data can be found [here](../rl_cartpole_batch_coach/rl_cartpole_batch_coach.ipynb).)\n",
    "\n",
    "1. *Objective*: Prevent the pole from falling over\n",
    "2. *Environment*: The environment used in this exmaple is part of OpenAI Gym, corresponding to the version of the cart-pole problem described by Barto, Sutton, and Anderson [1]\n",
    "3. *State*: Cart position, cart velocity, pole angle, pole velocity at tip\t\n",
    "4. *Action*: Push cart to the left, push cart to the right\n",
    "5. *Reward*: Reward is 1 for every step taken, including the termination step\n",
    "\n",
    "References\n",
    "\n",
    "1. AG Barto, RS Sutton and CW Anderson, \"Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problem\", IEEE Transactions on Systems, Man, and Cybernetics, 1983."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46226847",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "In this notebook, we aim to show the whole process of using SageMaker and Ray to train RL agent.\n",
    "1. Setup the pre-required dependencies\n",
    "2. Initialize the Ray cluster\n",
    "3. Initialize the RL agent\n",
    "4. Train the agent in a distributed fashion using CPUs and GPUs provided by the SageMaker nootbook. \n",
    "5. Save/restore/evaluate the agent \n",
    "6. Tune the agent by trying different combinations of hyperparameters of the agent\n",
    "7. Find the best hyperparameters, restore/evaluate the tuned agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a66ff8",
   "metadata": {},
   "source": [
    "## Pre-requisites \n",
    "### Install dependencies\n",
    "To get started, we need to install libraries as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d949692d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U 'ray[rllib, tune, serve]'\n",
    "!pip install gym[atari] autorom[accept-rom-license]\n",
    "!pip install box2d-py\n",
    "!pip install pygame\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de729cbf",
   "metadata": {},
   "source": [
    "### Imports\n",
    "We'll import the Python libraries as needed, set up the environment with a few prerequisites for permissions and configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961d44a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import os \n",
    "import tqdm\n",
    "import gym\n",
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer, DEFAULT_CONFIG\n",
    "from ray.tune.logger import pretty_print\n",
    "import logging\n",
    "from ray.air.config import RunConfig, ScalingConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b83695c",
   "metadata": {},
   "source": [
    "### CPU/GPU of the notebook in use (ml.p3.2xlarge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b5bf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_num = os.cpu_count()\n",
    "gpu_num = torch.cuda.device_count()\n",
    "print(cpu_num, 'CPUs')\n",
    "print(gpu_num, 'GPUs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b86ea9",
   "metadata": {},
   "source": [
    "### Start a Ray cluster at the notebook instance (ml.p3.2xlarge)\n",
    "Note that ray.shutdown() is called before ray.init(). This is used to avoid the error caused by calling ray.init() more than once by accident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accb79ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True, log_to_driver=False, logging_level=logging.FATAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b0db68",
   "metadata": {},
   "source": [
    "Ray automatically detects all available CPUs and GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d12de3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cpus = int(ray.available_resources()['CPU'])\n",
    "num_gpus = int(ray.available_resources()['GPU'])\n",
    "print('num_cpus', num_cpus)\n",
    "print('num_gpus', num_gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8410d9",
   "metadata": {},
   "source": [
    "### Set up configurations of Ray RL trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a71503",
   "metadata": {},
   "source": [
    "Define the deep learning framework: 'tf': tensorflow, 'torch': pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaf9e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = DEFAULT_CONFIG.copy()\n",
    "config['framework'] = 'torch'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f630e8",
   "metadata": {},
   "source": [
    "This set of configurations control the allocation of CPUs and GPUs. \n",
    "    Check the link for guideline https://docs.ray.io/en/latest/rllib/rllib-training.html#specifying-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199d551c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['num_workers'] = num_cpus-1  \n",
    "config['num_gpus'] = num_gpus #how many GPUs are assigned to the driver\n",
    "config['num_cpus_per_worker'] = 1 #how many CPU for each worker, at least 1\n",
    "config['num_gpus_per_worker'] = 0 #how many GPU for each worker, we set it as 0 since worker is only responsible for data collection rather than learning.\n",
    "config['num_envs_per_worker'] = 5 #how many envs interacts with by each worker. \n",
    "config['recreate_failed_workers'] = True # auto handle the worker failure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8143df9",
   "metadata": {},
   "source": [
    "This set of configurations control the training scheme of RL algorihtm (e.g., PPO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66388b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['train_batch_size'] = 5000\n",
    "config['num_sgd_iter'] = 10\n",
    "config['sgd_minibatch_size'] = 500\n",
    "# config['model']['fcnet_hiddens'] = [64, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf5a713",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['log_level'] = 'ERROR'\n",
    "config['create_env_on_driver'] = True # used for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2dd84d",
   "metadata": {},
   "source": [
    "Define the environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1185cd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_name = 'Taxi-v3' \n",
    "env_name = 'CartPole-v1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0977f2",
   "metadata": {},
   "source": [
    "Initialize the agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97f5a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PPOTrainer(config=config,  env=env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce6a937",
   "metadata": {},
   "source": [
    "### Train the agent. Training results are automatically logged in '~/ray_results/' by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b71cdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print('training ....')\n",
    "rewards = []\n",
    "for i in tqdm.tqdm(range(50)):\n",
    "    result = agent.train()\n",
    "    rewards.append(result['episode_reward_mean'])\n",
    "    print(\"iteration {:3d} reward {:6.2f}\".format(i+1,result['episode_reward_mean']))\n",
    "    ## save checkpoints perodically.\n",
    "    if i % 10 == 0 and i > 0:\n",
    "        checkpoint_path = agent.save()\n",
    "        print('iteration %s checkpoint saved at'%(i), checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbc3bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'reward': rewards})\n",
    "df.to_csv(env_name+'_rewards_num_worker_%s.csv'%(int(config['num_workers'])), index=False)\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(range(df.shape[0]), df['reward'].values, label='num_workers=%s'%(num_cpus-1))\n",
    "plt.xlabel('training iterations', fontsize=12)\n",
    "plt.ylabel('reward/episode', fontsize=12)\n",
    "plt.legend(loc=0, fontsize=12)\n",
    "plt.title(env_name, fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(env_name+'_rewards_num_worker_%s'%(int(config['num_workers']))+'.png', dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be875e58",
   "metadata": {},
   "source": [
    "Show the neural network structure of the agent policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0245d271",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = agent.get_policy()\n",
    "model = policy.model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87164027",
   "metadata": {},
   "source": [
    "Save the trained agent as a checkpoint by calling agent.save() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7a560c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Trained Model as a check point\n",
    "checkpoint_path = agent.save()\n",
    "print(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de141f2d",
   "metadata": {},
   "source": [
    "### Load the checkpoint and evaluate the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b7e4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = agent.evaluate(checkpoint_path)\n",
    "print(pretty_print(evaluation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90a713f",
   "metadata": {},
   "source": [
    "### Hyperparameters tuning. \n",
    "1. param_space defines the hyperparameters that we would like to tune\n",
    "    e.g., 'num_sdg_iter', 'sdg_mini_batchsize''train_batch_size' etc..\n",
    "3. Ray will automatically schedule all available workers to run tunning trails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a95eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()\n",
    "ray.init(log_to_driver=False, logging_level=logging.FATAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a7fa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from ray import air, tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea80cd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = tune.Tuner(\n",
    "    'PPO',\n",
    "    run_config=air.RunConfig(stop={\"training_iteration\": 100}, \n",
    "                             verbose=0,\n",
    "                            ),\n",
    "    \n",
    "    tune_config=tune.TuneConfig(\n",
    "        metric=\"episode_reward_mean\",\n",
    "        mode=\"max\",\n",
    "        num_samples=10, \n",
    "    ),\n",
    "    \n",
    "    param_space={\n",
    "        'env': 'CartPole-v1',\n",
    "        'framework': 'torch',\n",
    "        \"num_sgd_iter\": tune.choice([10, 20]), # tune.uniform()/tune.grid_search()\n",
    "        \"sgd_minibatch_size\": tune.choice([128, 256]),\n",
    "        \"train_batch_size\": tune.choice([500, 1000]),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a52ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_results = tuner.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05c9ba0",
   "metadata": {},
   "source": [
    "### Organize the tunning results in a dataframe and sort rows by filter metric e.g., episode_reward_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28107ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tune_results.get_dataframe(filter_metric=\"episode_reward_mean\", filter_mode=\"max\")\n",
    "df[['episode_reward_mean', 'config/train_batch_size', 'config/num_sgd_iter','config/sgd_minibatch_size']].sort_values(by='episode_reward_mean', ascending=False)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f570e9d4",
   "metadata": {},
   "source": [
    "## Retrieve the best hyperparameters. Restore and Evaluate the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d693ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result = tune_results.get_best_result(metric='episode_reward_mean')\n",
    "best_checkpoint = best_result.checkpoint\n",
    "# best_config = best_result.config\n",
    "# # print(pretty_print(best_config))\n",
    "\n",
    "evaluation = agent.evaluate(best_checkpoint)\n",
    "print(pretty_print(evaluation))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
