{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ac4a9b9",
   "metadata": {},
   "source": [
    "# Solve Super-Mario-Bros with SageMaker RL + Ray\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e296e5fc",
   "metadata": {},
   "source": [
    "![](../xxx-rl-mario-ray/mario.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dbcc23",
   "metadata": {},
   "source": [
    "---\n",
    "## Introduction\n",
    "\n",
    "In this notebook we'll start from the cart-pole balancing problem, where a pole is attached by an un-actuated joint to a cart, moving along a frictionless track. Instead of applying control theory to solve the problem, this example shows how to solve the problem with reinforcement learning on Amazon SageMaker and Ray RLlib. You can choose either TensorFlow or PyTorch as your underlying DL framework.\n",
    "\n",
    "(For a similar example using Coach library, see this [link](../rl_cartpole_coach/rl_cartpole_coach_gymEnv.ipynb). Another Cart-pole example using Coach library and offline data can be found [here](../rl_cartpole_batch_coach/rl_cartpole_batch_coach.ipynb).)\n",
    "\n",
    "1. *Objective*: Prevent the pole from falling over\n",
    "2. *Environment*: The environment used in this exmaple is part of OpenAI Gym, corresponding to the version of the cart-pole problem described by Barto, Sutton, and Anderson [1]\n",
    "3. *State*: Cart position, cart velocity, pole angle, pole velocity at tip\t\n",
    "4. *Action*: Push cart to the left, push cart to the right\n",
    "5. *Reward*: Reward is 1 for every step taken, including the termination step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13badaaf",
   "metadata": {},
   "source": [
    "## Pre-requisites \n",
    "### Install dependencies\n",
    "To get started, we need to install libraries as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9520ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U 'ray[rllib, tune, serve]'\n",
    "# !pip install gym[atari] autorom[accept-rom-license]\n",
    "# !pip install box2d-py\n",
    "# !pip install pygame\n",
    "# !pip install tqdm\n",
    "# !pip install gym-super-mario-bros\n",
    "# !pip install ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4e4c50",
   "metadata": {},
   "source": [
    "### Imports\n",
    "We'll import the Python libraries as needed, set up the environment with a few prerequisites for permissions and configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833bbb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import os\n",
    "num_gpus = torch.cuda.device_count()\n",
    "num_cpus = os.cpu_count()\n",
    "print('GPUs', num_gpus)\n",
    "print('CPUs', num_cpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8edbe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm \n",
    "import logging\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT,COMPLEX_MOVEMENT\n",
    "import ray\n",
    "from ray import tune, air\n",
    "from ray.tune.registry import register_env\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.rllib.agents.ppo import PPOTrainer, DEFAULT_CONFIG\n",
    "from ray.rllib.algorithms.impala import Impala, ImpalaConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efbef5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from ray.rllib.env.wrappers.atari_wrappers import (MonitorEnv,\n",
    "                                          NoopResetEnv,\n",
    "                                          WarpFrame,\n",
    "                                          FrameStack)\n",
    "\n",
    "\n",
    "class EpisodicLifeEnv(gym.Wrapper):\n",
    "\n",
    "    def __init__(self, env):\n",
    "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game\n",
    "        over. Done by DeepMind for the DQN and co. since it helps value\n",
    "        estimation.\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.lives = 0\n",
    "        self.was_real_done = True\n",
    "\n",
    "   \n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        self.was_real_done = done\n",
    "        # check current lives, make loss of life terminal,\n",
    "        # then update lives to handle bonus lives\n",
    "        lives = self.env.unwrapped._life\n",
    "        if self.lives > lives > 0:\n",
    "            # for Qbert sometimes we stay in lives == 0 condtion for a few fr\n",
    "            # so its important to keep lives > 0, so that we only reset once\n",
    "            # the environment advertises done.\n",
    "            done = True\n",
    "        self.lives = lives\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"Reset only when lives are exhausted.\n",
    "        This way all states are still reachable even though lives are episodic,\n",
    "        and the learner need not know about any of this behind-the-scenes.\n",
    "        \"\"\"\n",
    "        if self.was_real_done:\n",
    "            obs = self.env.reset(**kwargs)\n",
    "        else:\n",
    "            # no-op step to advance from terminal/lost life state\n",
    "            obs, _, _, _ = self.env.step(0)\n",
    "        self.lives = self.env.unwrapped._life\n",
    "        return obs\n",
    "\n",
    "\n",
    "    \n",
    "class CustomReward(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super(CustomReward, self).__init__(env)\n",
    "        self._current_score = 0\n",
    "\n",
    "    def step(self, action):\n",
    "        state, reward, done, info = self.env.step(action)\n",
    "        reward += (info['score'] - self._current_score) / 40.0\n",
    "        self._current_score = info['score']\n",
    "        if info['time']<=300:\n",
    "            done = True\n",
    "        if done:\n",
    "            if info['flag_get']:\n",
    "                reward += 350.0\n",
    "#             else:\n",
    "#                 reward -= 50.0\n",
    "        return state, reward / 10.0, done, info\n",
    "\n",
    "\n",
    "def env_creator(env_name):\n",
    "    env = gym_super_mario_bros.make(env_name)\n",
    "    env = CustomReward(env)\n",
    "    env = JoypadSpace(env, COMPLEX_MOVEMENT)\n",
    "    env = MonitorEnv(env)\n",
    "    env = NoopResetEnv(env, noop_max=30)\n",
    "    env = EpisodicLifeEnv(env)\n",
    "    env = WarpFrame(env, 84)\n",
    "    env = FrameStack(env, 4)     \n",
    "    return env\n",
    "\n",
    "\n",
    "def print_results(result, iteration, config):\n",
    "    table = [['IMPALA',\n",
    "              config['num_gpus'],\n",
    "              config['num_workers'],\n",
    "              config['num_envs_per_worker'],\n",
    "              iteration,\n",
    "              result['episodes_total'],\n",
    "              result['timesteps_total'],\n",
    "              result['episode_len_mean'],\n",
    "              round(result['episode_reward_mean'], 3)]]\n",
    "\n",
    "    print(tabulate(table, headers=['Agent',\n",
    "                                    'GPUs',\n",
    "                                    'Workers',\n",
    "                                    'Envs per Worker',\n",
    "                                    'Iteration',\n",
    "                                    'Episodes',\n",
    "                                    'Steps',\n",
    "                                    'Episode Length(mean)',\n",
    "                                    'Mean Reward'],\n",
    "                        tablefmt='psql',\n",
    "                        showindex=\"never\"))\n",
    "    print()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51822cd5",
   "metadata": {},
   "source": [
    "### Create and register environment to Ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adea7809",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'SuperMarioBros-v0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c99d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_creator_lambda(config):\n",
    "    return env_creator(env_name)\n",
    "\n",
    "register_env(env_name, env_creator_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5721d6ad",
   "metadata": {},
   "source": [
    "## Initialize Ray Cluster for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e769d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True, log_to_driver=False, logging_level=logging.FATAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab49a230",
   "metadata": {},
   "source": [
    "## Initialize RL algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1821d3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'env': env_name,\n",
    "    \"framework\": \"torch\",\n",
    "    \"num_workers\": num_cpus-1,\n",
    "    'num_gpus': num_gpus,\n",
    "    'train_batch_size': 5000,\n",
    "    'recreate_failed_workers': True,\n",
    "    'num_envs_per_worker': 1,\n",
    "    'log_level': 'ERROR',\n",
    "    'create_env_on_driver': True,\n",
    "    \"evaluation_num_workers\": 1,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c002d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent =Impala(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f12483",
   "metadata": {},
   "source": [
    "### Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32d75bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rewards = []\n",
    "episodes = []\n",
    "steps = []\n",
    "episode_lens = []\n",
    "\n",
    "iter_num = 1000\n",
    "for iteration in tqdm(range(iter_num)):\n",
    "    result = agent.train()\n",
    "    # record the learning process\n",
    "    rewards.append(result['episode_reward_mean'])\n",
    "    print_results(result, iteration, config)\n",
    "    pretty_print(result['perf'])\n",
    "    # save the check point every 500 training iterations\n",
    "    if iteration % 500 == 0 and iteration > 0:\n",
    "        checkpoint = agent.save()\n",
    "        print('Checkpoint saved at', checkpoint)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb05525",
   "metadata": {},
   "source": [
    "### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07c52a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(range(iter_num), rewards)\n",
    "plt.xlabel('Training Iteration', fontsize=12)\n",
    "plt.ylabel('epoisode reward (mean)', fontsize=12)\n",
    "plt.title('SuperMario', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/super-mario-reward_gpus_%s_workers_%s'%(num_gpus, config['num_workers'])+'.png', dpi=100)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5056eb7f",
   "metadata": {},
   "source": [
    "### Evaluate "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cbfd6f",
   "metadata": {},
   "source": [
    "![](../xxx-rl-mario-ray/recordings/record_episode_0.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6feedc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c796a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f9fb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_orig_env(env_name):\n",
    "    env = gym_super_mario_bros.make(env_name)\n",
    "    env = CustomReward(env)\n",
    "    env = JoypadSpace(env, COMPLEX_MOVEMENT)\n",
    "    return env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff0cd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = env_creator('SuperMarioBros-v0')\n",
    "orig_env = get_orig_env('SuperMarioBros-v0')\n",
    "\n",
    "epi_num = 10\n",
    "\n",
    "for epi in tqdm(range(epi_num)):\n",
    "    frame_list = []\n",
    "    cum_reward = 0\n",
    "    done = False\n",
    "    _ = orig_env.reset()\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    while not done:\n",
    "        step +=1 \n",
    "        action = agent.compute_single_action(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        cum_reward += reward \n",
    "        image, _, _, _ = orig_env.step(action)\n",
    "        frame = Image.fromarray(image, mode='RGB')\n",
    "        frame_list.append(frame)\n",
    "    print('episode %s, reward %s, step_num %s'%(epi, cum_reward, step))\n",
    "    frame_list[0].save(\n",
    "        \"records/record_episode_%s.gif\"%(epi), save_all=True, append_images=frame_list[::5], duration=1, loop=1\n",
    "    )\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c9900d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aed1088",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9698bb3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53fa692",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fbf4f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b67719e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e58bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbe2401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffb72a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe3ebb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
